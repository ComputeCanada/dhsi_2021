#+title: Natural Language Processing
#+subtitle: Lesson Notes
#+slug: nlp

{{<cor>}}Wednesday, June 16{{</cor>}}

{{<cgr>}}11:15 am–1:00 pm{{</cgr>}} \\
*Connection Details:* {{<a "https://us02web.zoom.us/j/87259243311?pwd=RlhnU2huTGFpTFhwN1p5ZnpXcWEvdz0" "Lesson Room Link">}}— Go to Room 1 (NLP)

{{<br size="1">}}

#+BEGIN_box
*Heads-up*

This session is conceptual rather than hands-on.\\
I will explain what machine learning, neural networks, deep learning, and NLP are and I will demo an example of using NLP for sentiment analysis, but you will not get to play with Python code.
#+END_box

* Resources

- {{<a "https://stanfordnlp.github.io/stanza/" "Stanza">}} is a Python NLP package developed by the {{<a "https://nlp.stanford.edu/" "Stanford NLP Group">}}
- Since many of you have interests in Russian literature, {{<a "https://russiannlp.sites.stanford.edu/resources" "here">}} is a set of interesting resources
- {{<a "https://web.stanford.edu/~jurafsky/slp3/" "Speech and Language Processing">}} is a book by {{<a "https://web.stanford.edu/~jurafsky/" "Dan Jurafsky">}} and {{<a "https://home.cs.colorado.edu/~martin/" "James Martin">}} with a free online version
- The {{<a "http://web.stanford.edu/class/cs224n/" "CS224n: Natural Language Processing with Deep Learning">}} Stanford University course has many resources available on its site

* Concepts

/Click below to open the presentation.\\
Once in the presentation, navigate the slides with the left and right arrows and refresh any page to show the link that will take you back here./

#+BEGIN_export html
<a href="https://westgrid-slides.netlify.app/nlp/#/"><p align="center"><img src="/img/nlp_slides.png" title="" width="100%" style="border-style: solid; border-width: 1.5px 1.5px 0 2px; border-color: black"/></p></a>
#+END_export
  
* Example

Let's go through an example of NLP using {{<a "https://github.com/fastai/fastai" "fastai.">}} I am using here one of their demo examples.

Below is the code that I will run. Because we only have one GPU and because it would take an extremely long time to run this code on CPUs, unfortunately, you will have to watch and you won't be able to run this on your JupyterLab.

#+BEGIN_src python
# First, we load the necessary functions from fastai
# Note that import * is appropriate with fastai
# but not a very good practice with other packages
from fastai.text.all import *

# Then we build our data loaders
# This is an object of the fastai class DataLoaders
# It contains a validation data loader and a training data loader
dls = TextDataLoaders.from_folder(
    '/project/def-sponsor00/dhsi/.fastai/data/imdb',
    valid='test'
)

# Let's have a look at the data
dls.show_batch()

# Now, we define our learner
# AWD_LSTM is the architecture (https://arxiv.org/abs/1708.02182)
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)

# Finally, we fine-tune the pretrained model with our data
# over 2 epochs with a learning rate of 1e-2
learn.fine_tune(2, 1e-2)

# Then we can use our model to predict whether sentences are positive or negative
learn.predict("This is amazing.")
learn.predict("I love it.")
learn.predict("This is terrible.")
#+END_src

* I don't understand what neural networks really are!

If you want to really understand what neural networks are and how they work, I really recommend you to watch the 4 videos below from {{<a "https://www.3blue1brown.com/" "3Blue1Brown by Grant Sanderson">}}. They are easy to watch, fun, and do an excellent job at introducing the functioning of a simple neural network.

*** What neural networks really are{{<2br>}}

{{<youtube aircAruvnKk>}} {{<br>}}

#+BEGIN_note
For those of you who will develop NLP models, if you find that your mathematical background is shaky, 3blue1brown also has [[https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab][an excellent series of videos on linear algebra]] and [[https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr][an equally great one on calculus]].
#+END_note

*** How neural networks learn{{<2br>}}

{{<youtube IHZwWFHWa-w>}}

*** More technical: how backpropagation works{{<2br>}}

{{<youtube Ilg3gGewQ5U>}}{{<br>}}

#+BEGIN_note
There is one minor terminological error in this video: they call the use of mini-batches *stochastic gradient descent*. In fact, this is called *mini-batch gradient descent*. Stochastic gradient descent uses a single example at each iteration.
#+END_note

*** Even more nerdy: the calculus of backpropagation{{<2br>}}

{{<youtube tIeHLnjs5U8>}}
